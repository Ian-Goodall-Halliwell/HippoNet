{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc6cc61-582d-4f67-af4a-21e4c2840f08",
   "metadata": {},
   "source": [
    "# AtariSpace\n",
    "\n",
    "This task will not \"play\" atari but rather simply explore it. That is, there are no rewards, just a (nearly) passive viewing of the different games. The network can take actions within a game, or can cycle between the list of games, but is not rewarded by game score. Instead the network simply watches the games and tries to predict future frames. \n",
    "\n",
    "## VNet\n",
    "This \"visual\" network is simply an autoencoder. The bottleneck layer is treated as a latent variable (LV). It is convolutional with tied encoder-decoder weights. \n",
    "\n",
    "## HNet\n",
    "This \"hippocampal\" network takes in the current LV and tries to predict the next LV. This assumes that VNet is relatively constant (low learning rate or frozen). This should have a high LR, and use memory augmentation to preserve long-term dependencies.\n",
    "\n",
    "## MNet\n",
    "A small \"motor\"  network will be trained to maximize prediction errors as a method to encourage exploration. Prediction error is a positive scalar and there is an ideal \"Goldilocks zone\" for exploration which we approximate with an Erlang distribution.\n",
    "This could just be random actions, but if any useful exploration policies are learned then great. \n",
    "\n",
    "## Environment notes\n",
    "Any input from any game is allowed (0-17), and ideally MNet should learn which inputs are actually useful in a given game. Two more inputs will allow for cycling between games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b090a1-6e54-48a1-85fd-d6c78481877b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export03/data/opt/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random, datetime, os, copy, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9097ae03-3d44-48b8-b2d0-933da194a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = 17 # possible actions in all games (inc 0)\n",
    "games = ['ALE/Tetris-v5', 'ALE/Adventure-v5', 'ALE/AirRaid-v5', 'ALE/Alien-v5', 'ALE/Amidar-v5', 'ALE/Assault-v5', 'ALE/Asterix-v5', \n",
    "         'ALE/Asteroids-v5', 'ALE/Atlantis-v5', 'ALE/Atlantis2-v5', 'ALE/Backgammon-v5', 'ALE/BankHeist-v5', 'ALE/BasicMath-v5', \n",
    "         'ALE/BattleZone-v5', 'ALE/BeamRider-v5', 'ALE/Berzerk-v5', 'ALE/Blackjack-v5', 'ALE/Bowling-v5', 'ALE/Boxing-v5', 'ALE/Breakout-v5', \n",
    "         'ALE/Carnival-v5', 'ALE/Casino-v5', 'ALE/Centipede-v5', 'ALE/ChopperCommand-v5', 'ALE/CrazyClimber-v5', 'ALE/Crossbow-v5', \n",
    "         'ALE/Darkchambers-v5', 'ALE/Defender-v5', 'ALE/DemonAttack-v5', 'ALE/DonkeyKong-v5', 'ALE/DoubleDunk-v5', 'ALE/Earthworld-v5', \n",
    "         'ALE/ElevatorAction-v5', 'ALE/Enduro-v5', 'ALE/Entombed-v5', 'ALE/Et-v5', 'ALE/FishingDerby-v5', 'ALE/FlagCapture-v5', \n",
    "         'ALE/Freeway-v5', 'ALE/Frogger-v5', 'ALE/Frostbite-v5', 'ALE/Galaxian-v5', 'ALE/Gopher-v5', 'ALE/Gravitar-v5', 'ALE/Hangman-v5', \n",
    "         'ALE/HauntedHouse-v5', 'ALE/Hero-v5', 'ALE/HumanCannonball-v5', 'ALE/IceHockey-v5', 'ALE/Jamesbond-v5', 'ALE/JourneyEscape-v5', \n",
    "         'ALE/Kaboom-v5', 'ALE/Kangaroo-v5', 'ALE/KeystoneKapers-v5', 'ALE/KingKong-v5', 'ALE/Klax-v5', 'ALE/Koolaid-v5', 'ALE/Krull-v5', \n",
    "         'ALE/KungFuMaster-v5', 'ALE/LaserGates-v5', 'ALE/LostLuggage-v5', 'ALE/MarioBros-v5', 'ALE/MiniatureGolf-v5', 'ALE/MontezumaRevenge-v5', \n",
    "         'ALE/MrDo-v5', 'ALE/MsPacman-v5', 'ALE/NameThisGame-v5', 'ALE/Othello-v5', 'ALE/Pacman-v5', 'ALE/Phoenix-v5', 'ALE/Pitfall-v5', \n",
    "         'ALE/Pitfall2-v5', 'ALE/Pong-v5', 'ALE/Pooyan-v5', 'ALE/PrivateEye-v5', 'ALE/Qbert-v5', 'ALE/Riverraid-v5', 'ALE/RoadRunner-v5', \n",
    "         'ALE/Robotank-v5', 'ALE/Seaquest-v5', 'ALE/SirLancelot-v5', 'ALE/Skiing-v5', 'ALE/Solaris-v5', 'ALE/SpaceInvaders-v5', \n",
    "         'ALE/SpaceWar-v5', 'ALE/StarGunner-v5', 'ALE/Superman-v5', 'ALE/Surround-v5', 'ALE/Tennis-v5', \n",
    "         'ALE/TimePilot-v5', 'ALE/Trondead-v5', 'ALE/Turmoil-v5', 'ALE/Tutankham-v5', 'ALE/UpNDown-v5', 'ALE/Venture-v5', 'ALE/VideoCheckers-v5', \n",
    "         'ALE/VideoPinball-v5', 'ALE/WizardOfWor-v5', 'ALE/WordZapper-v5', 'ALE/YarsRevenge-v5', 'ALE/Zaxxon-v5']\n",
    "g=0\n",
    "def AtariSpace(action):\n",
    "    global env,g\n",
    "    if action==0:\n",
    "        g = g-1\n",
    "        if g<0:\n",
    "            g=len(games)-1\n",
    "        env = gym.make(games[g], obs_type='grayscale', \n",
    "                       render_mode='rgb_array', \n",
    "                       full_action_space=True)\n",
    "        frame = env.reset()[0]\n",
    "    elif action==1:\n",
    "        g = g+1\n",
    "        if g>len(games)-1:\n",
    "            g=0\n",
    "        env = gym.make(games[g], obs_type='grayscale', \n",
    "                       render_mode='rgb_array', \n",
    "                       full_action_space=True)\n",
    "        frame = env.reset()[0]\n",
    "    else:\n",
    "        frame = env.step(action-2)[0]\n",
    "    if np.max(frame)==0: # some games have a blank frame\n",
    "        frame = env.step(0)[0]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5778ea-813a-4946-8c6d-1c52d8fab6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "current_frame = torch.zeros((241,153)).to(device)\n",
    "def frameproc(frame):\n",
    "    global current_frame\n",
    "    # trim, resize to suit our convolutions, and standardize\n",
    "    # frame = frame[25:225,:]\n",
    "    frame = np.expand_dims(np.expand_dims(frame, 0), 0)\n",
    "    frame = torch.as_tensor(frame, dtype=torch.float32)\n",
    "    means = frame.mean()\n",
    "    stds = frame.std()\n",
    "    frame = (frame - means) / stds\n",
    "    frame = torchvision.transforms.Resize((241,153))(frame)\n",
    "    current_frame = frame.to(device)\n",
    "    return\n",
    "\n",
    "def live_plot(imgs):\n",
    "    clear_output(wait=True)\n",
    "    for p in range(len(imgs)):\n",
    "        plt.subplot(1,len(imgs),p+1)\n",
    "        if current_frame.dtype==torch.float32:\n",
    "            plt.imshow(torch.squeeze(imgs[p]).to('cpu').detach())\n",
    "        else:\n",
    "            plt.imshow(imgs[p])\n",
    "        plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b48d5a3-fc0e-4d7e-99f8-333aa1ef4177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility layers for nn.Sequential\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "    https://discuss.pytorch.org/t/where-is-the-noise-layer-in-pytorch/2887/4\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.5, is_relative_detach=True):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "        self.noise = torch.tensor(0).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma != 0:\n",
    "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            sampled_noise = self.noise.repeat(*x.size()).float().normal_() * scale\n",
    "            x = x + sampled_noise\n",
    "        return x \n",
    "    \n",
    "class artanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return 0.5*(torch.log(1+x)/(1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e0bdef-0272-488e-8a14-982494c74b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 241, 153])\n",
      "torch.Size([1, 1, 241, 153])\n"
     ]
    }
   ],
   "source": [
    "# Test out environment and utilities!\n",
    "for s in [1,1,2,1,1,1,0,0,0,2,0,0]:\n",
    "    frame = AtariSpace(s)\n",
    "    origshape = current_frame.shape\n",
    "    frameproc(frame)\n",
    "    live_plot([current_frame])\n",
    "    time.sleep(0.5)\n",
    "print(origshape)\n",
    "print(current_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29400acd-dbab-453e-9552-e7bba1ba8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119.0, 75.0)\n",
      "(59.0, 37.0)\n",
      "(29.0, 18.0)\n",
      "16704\n"
     ]
    }
   ],
   "source": [
    "# conv2d design testing\n",
    "# ideally out convolutions should work with the input image size to only produce integers here. This will help a lot in the ConvTranspose2d decoder!\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1\n",
    "    w = ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1\n",
    "    return h, w\n",
    "t1 = conv_output_shape(current_frame.shape[2:4],kernel_size=5, stride=2)\n",
    "t2 = conv_output_shape(t1,kernel_size=3, stride=2)\n",
    "t3 = conv_output_shape(t2,kernel_size=3, stride=2)\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "flatsz = int(np.prod(t3)*32)\n",
    "print(flatsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71efbc7e-5cff-4059-8058-33abba705f05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 638.00 MiB (GPU 0; 10.92 GiB total capacity; 202.00 KiB already allocated; 7.38 MiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrozen\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderfrozen(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m vnet \u001b[38;5;241m=\u001b[39m \u001b[43mVNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(vnet)\n",
      "File \u001b[0;32m/export03/data/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export03/data/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/export03/data/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/export03/data/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/export03/data/opt/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 638.00 MiB (GPU 0; 10.92 GiB total capacity; 202.00 KiB already allocated; 7.38 MiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "class VNet(nn.Module):\n",
    "    #This will be a convolutional AE with tied encoder-decoder weights. Encoder can optionally be frozen.\n",
    "\n",
    "    def __init__(self, output_dim=(512)):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flatsz, 10000, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10000, output_dim, bias=False),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, 10000, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10000,flatsz,bias=False),\n",
    "            nn.Tanh(),\n",
    "            # nn.Unflatten(1,(1,241,153)),\n",
    "            nn.Unflatten(1,(32,int(t3[0]),int(t3[1]))),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=5, stride=2),\n",
    "        )\n",
    "            \n",
    "        # tie the weights\n",
    "        for i in range(len(self.encoder)):\n",
    "            ii = len(self.encoder)-i-1\n",
    "            if hasattr(self.encoder[i],'weight'):\n",
    "                if len(self.encoder[i].weight.shape)>2:\n",
    "                    self.decoder[ii].weight = nn.Parameter(self.encoder[i].weight)\n",
    "                else:\n",
    "                    self.decoder[ii].weight = nn.Parameter(self.encoder[i].weight.t())\n",
    "\n",
    "        # frozen versions\n",
    "        self.encoderfrozen = copy.deepcopy(self.encoder)\n",
    "        for p in self.encoderfrozen.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.decoderfrozen = copy.deepcopy(self.decoder)\n",
    "        for p in self.decoderfrozen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.encoder(input)\n",
    "        elif model == \"frozen\":\n",
    "            return self.encoderfrozen(input)\n",
    "        \n",
    "    def backward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.decoder(input)\n",
    "        elif model == \"frozen\":\n",
    "            return self.decoderfrozen(input)\n",
    "vnet = VNet().to(device)\n",
    "print(vnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32646be-e3d4-4b6e-9eba-ed60aed45641",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLoss = nn.MSELoss()\n",
    "VOptim = optim.Adam(vnet.parameters(), lr=0.0001, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc874347-280f-4308-b950-57518b159c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnet.load_state_dict(torch.load('VNet_saved'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc95c7e-86a7-427d-a902-bbd4c7e23559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test / pretrain of VNet\n",
    "\n",
    "# log = []\n",
    "# for i in range(100000):\n",
    "#     frame = AtariSpace(1+possible_actions)\n",
    "#     frameproc(frame)\n",
    "\n",
    "#     next_LV = vnet.forward(current_frame, model='online')\n",
    "#     recon_frame = vnet.backward(next_LV, model='online')\n",
    "\n",
    "#     vloss = VLoss(recon_frame,current_frame)\n",
    "#     log.append([vloss.to('cpu').detach()])\n",
    "#     VOptim.zero_grad()\n",
    "#     vloss.backward()\n",
    "#     VOptim.step()\n",
    "    \n",
    "#     if i%100==0:\n",
    "#         live_plot([current_frame,recon_frame.detach()])\n",
    "#         print(f'iteration: {i}')\n",
    "        \n",
    "#     del next_LV,recon_frame,vloss,frame\n",
    "    \n",
    "# torch.save(vnet.state_dict(),'VNet_saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ead0d-08c8-47d9-9eab-de5ee74bf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNet(nn.Module):\n",
    "    # currently just fully connected (but high LR)\n",
    "    \n",
    "    def __init__(self, input_dim=(512+3), output_dim=(512)):\n",
    "        super().__init__()\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.frozen = copy.deepcopy(self.online)\n",
    "        for p in self.frozen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"frozen\":\n",
    "            return self.frozen(input)\n",
    "        \n",
    "hnet = HNet().to(device)\n",
    "print(hnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26a45a-8284-4db6-9de3-6b8d9a8a5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLoss = nn.MSELoss()\n",
    "HOptim = optim.SGD(hnet.parameters(),lr=0.1, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f32c00-7b08-4dff-b57d-6a9c43cc69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNet(nn.Module):\n",
    "    # simple fully connected with softmax at the end; optionally frozen\n",
    "    \n",
    "    def __init__(self, input_dim=(512), output_dim=(3)):\n",
    "        super().__init__()\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            #nn.Tanh(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            GaussianNoise(),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.frozen = copy.deepcopy(self.online)\n",
    "        for p in self.frozen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"frozen\":\n",
    "            return self.frozen(input)\n",
    "        \n",
    "mnet = MNet().to(device)\n",
    "print(mnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d4b4c-74d0-4c02-809f-4c377b93eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "def erlang(x,k=2,l=2):\n",
    "    return ((l**k)*(x**(k-1))*(math.e**(-l*x)))/math.factorial(k-1)*2 -1\n",
    "def MLoss(action,x):\n",
    "    global m\n",
    "    mloss = torch.sum(action)*(erlang(x-m)) # note action sums to 1\n",
    "    m = m+(0.01*mloss.detach()) # \"sticky\" running average so we know if x was good relative to its temporal nieghbours\n",
    "    return mloss\n",
    "x = np.linspace(0,5,num=50)\n",
    "plt.plot(x,erlang(x))\n",
    "plt.title(\"Goldilocks novelty zone\")\n",
    "plt.xlabel(\"novelty (prediction error)\")\n",
    "plt.ylabel(\"motor policy loss\")\n",
    "\n",
    "MOptim = optim.SGD(mnet.parameters(), lr=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103ba87-ce39-42f6-9626-8833bec6845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize state\n",
    "frame = AtariSpace(19)\n",
    "frameproc(frame)\n",
    "current_LV = vnet.forward(current_frame, model='frozen')\n",
    "\n",
    "# the complicated part.\n",
    "# 0) reconstruct the frame from the current LV (just for viewing)\n",
    "# 1a) choose next action from the current LV\n",
    "# 1b) make a prediction from the current LV about the next LV\n",
    "# 1c) reconstruct the next frame from the predicted LV\n",
    "# 2) take the action and get the actual next frame+LV\n",
    "# 3a) train VNet to reconstruct the current frame from the current LV\n",
    "# 3b) train HNet to predict next LV from the current LV\n",
    "# 3c) train MNet to maximize HLoss\n",
    "log = []\n",
    "t = time.time()\n",
    "for i in range(10000):\n",
    "    \n",
    "    current_action = mnet.forward(current_LV.detach(), model='online')\n",
    "    predicted_LV = hnet.forward(torch.cat((current_LV.detach(),\n",
    "                                           current_action.detach()),dim=1),model='online')\n",
    "    recon_frame = vnet.backward(current_LV.detach(), model='online')\n",
    "    predicted_frame = vnet.backward(predicted_LV.detach(), model='online')\n",
    "    \n",
    "    vloss = VLoss(recon_frame,current_frame)\n",
    "    # VOptim.zero_grad()\n",
    "    # vloss.backward()\n",
    "    # VOptim.step()\n",
    "    \n",
    "    old_frame = copy.deepcopy(current_frame.detach())\n",
    "    a = np.argmax(current_action.to('cpu').detach())\n",
    "    frame = AtariSpace(a)\n",
    "    #frame = AtariSpace(19) # just cycle through\n",
    "    frameproc(frame)\n",
    "    next_LV = vnet.forward(current_frame.detach(), model='online')\n",
    "    if i%100==0:\n",
    "        live_plot([old_frame,recon_frame,current_frame,predicted_frame])\n",
    "        print(i)\n",
    "   \n",
    "    hloss = HLoss(predicted_LV,next_LV.detach())\n",
    "    HOptim.zero_grad()\n",
    "    hloss.backward()\n",
    "    HOptim.step()\n",
    "    mloss = -MLoss(current_action,hloss.detach()/512)\n",
    "    MOptim.zero_grad()\n",
    "    mloss.backward()\n",
    "    MOptim.step()\n",
    "    \n",
    "    log.append([vloss.to('cpu').detach(), hloss.to('cpu').detach(), mloss.to('cpu').detach(), g])\n",
    "    current_LV = next_LV\n",
    "    del predicted_LV, recon_frame, predicted_frame, next_LV, hloss, vloss, mloss\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e0b2b-b3a5-43ae-9d02-517fb0edc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4), dpi=80)\n",
    "log = np.asarray(log)\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(np.convolve(np.array(log)[:,0],np.ones(100)/100, mode='valid'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(np.convolve(np.array(log)[:,1],np.ones(100)/100, mode='valid'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot(np.convolve(np.array(log)[:,2],np.ones(100)/100, mode='valid'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.scatter(range(len(log)),log[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f27ec2-3068-4d04-a3dd-3e5d31d4e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log[:,3], bins=len(games));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f3d48-bf42-4f42-86e5-d3dd948883da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
